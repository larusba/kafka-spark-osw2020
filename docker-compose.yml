version: "3"

services:
  zeppelin:
    hostname: zeppelin
    container_name: zeppelin
    image: apache/zeppelin:0.9.0
    depends_on:
      - neo4j
    ports:
      - "8080:8080"
      - "4040:4040"
    volumes:
      - ./zeppelin/notebook:/zeppelin/notebook
      - ./zeppelin/conf:/zeppelin/conf
      - ./zeppelin/jars:/jars
      - ./zeppelin/data:/zeppelin/spark-warehouse
      - ./zeppelin/interpreter/neo4j:/zeppelin/interpreter/neo4j

  neo4j:
    image: neo4j:4.1.4-enterprise
    hostname: neo4j
    container_name: neo4j
    ports:
      - "7474:7474"
      - "7687:7687"
    depends_on:
      - zookeeper
      - broker
    environment:
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      NEO4J_AUTH: neo4j/osw2020
      NEO4J_dbms_memory_heap_max__size: 8G
      NEO4J_dbms_security_procedures_unrestricted: "apoc.*"    
      NEO4J_dbms_logs_debug_level: DEBUG
      NEO4J_apoc_export_file_enabled: "true"
      NEO4J_kafka_zookeeper_connect: zookeeper:2181
      NEO4J_kafka_bootstrap_servers: broker:9093
      NEO4J_kafka_key_deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer
      NEO4J_kafka_value_deserializer: org.apache.kafka.common.serialization.ByteArrayDeserializer
      NEO4J_streams_source_enabled: "true"
      NEO4J_streams_sink_enabled: "true"
      NEO4J_streams_source_topic_nodes_customer: Customer{customerID,contactName,companyName}
      NEO4J_streams_source_topic_nodes_order: Order{orderID,shipCity,shipAddress}
      NEO4J_streams_source_topic_nodes_product: Product{productID,productName}
      NEO4J_streams_source_topic_nodes_people: Person{*}
      NEO4J_streams_source_topic_relationships_knows: KNOWS{*}
      NEO4J_streams_source_topic_relationships_purchased: PURCHASED{*}
      NEO4J_streams_source_topic_relationships_orders: ORDERS{quantity,unitPrice}
      NEO4J_streams_sink_topic_cypher_sales: "
        MERGE (c:Customer {customerID: event.customer.customerID})
        ON CREATE SET c.contactName = event.customer.contactName,
        c.companyName = event.customer.companyName
        MERGE (o:Order {orderID: event.order.orderID})
        ON CREATE SET o.shipCity = event.order.shipCity,
        o.shipAddress = event.order.shipAddress,
        o.orderDate = localdatetime()
        MERGE (p:Product {productID: event.product.productID})
        ON CREATE SET p.productName = event.product.productName
        MERGE (c)-[:PURCHASED]->(o)-[os:ORDERS {quantity: event.product.quantity, unitPrice: event.product.unitPrice}]->(p)"
      NEO4J_streams_sink_topic_cypher_pharma: "
        MERGE (p:Pharmacy{fiscalId: event.FISCAL_ID}) ON CREATE SET p.name = event.NAME
        MERGE (t:PharmacyType{type: event.TYPE_NAME})
        MERGE (a:Address{name: event.ADDRESS + ', ' + event.CITY})
        ON CREATE SET a.latitude = event.LATITUDE,
        a.longitude = event.LONGITUDE,
        a.code = event.POSTAL_CODE,
        a.point = event.POINT
        MERGE (c:City{name: event.CITY})
        MERGE (p)-[:IS_TYPE]-(t)
        MERGE (p)-[:HAS_ADDRESS]-(a)
        MERGE (a)-[:IS_LOCATED_IN]->(c)"
    volumes:
      - ./neo4j/plugins:/plugins

  zookeeper:
    image: confluentinc/cp-zookeeper
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-enterprise-kafka
    hostname: broker
    container_name: broker
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    expose:
      - "9093"
    environment:
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9093,OUTSIDE://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9093,OUTSIDE://0.0.0.0:9092
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:9093
      # workaround if we change to a custom name the schema_registry fails to start
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: "true"
      CONFLUENT_SUPPORT_CUSTOMER_ID: "anonymous"

  schema_registry:
    image: confluentinc/cp-schema-registry
    hostname: schema_registry
    container_name: schema_registry
    depends_on:
      - zookeeper
      - broker
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema_registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: "zookeeper:2181"

  connect:
    image: confluentinc/cp-kafka-connect
    hostname: connect
    container_name: connect
    depends_on:
      - zookeeper
      - broker
      - schema_registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "broker:9093"
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema_registry:8081"
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema_registry:8081"
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_ZOOKEEPER_CONNECT: "zookeeper:2181"
      CONNECT_PLUGIN_PATH: /usr/share/java,/tmp/connect-plugins,/usr/share/confluent-hub-components
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=DEBUG,org.I0Itec.zkclient=DEBUG,org.reflections=ERROR,org.apache.kafka.connect.transforms
    command:
      - bash
      - -c
      - |
        confluent-hub install --no-prompt neo4j/kafka-connect-neo4j:1.0.9
        confluent-hub install --no-prompt confluentinc/connect-transforms:latest
        /etc/confluent/docker/run

#  control-center:
#     image: confluentinc/cp-enterprise-control-center
#     hostname: control-center
#     container_name: control-center
#     depends_on:
#       - zookeeper
#       - broker
#       - schema_registry
#       - connect
#     ports:
#       - "9021:9021"
#     environment:
#       CONTROL_CENTER_BOOTSTRAP_SERVERS: 'broker:9093'
#       CONTROL_CENTER_ZOOKEEPER_CONNECT: 'zookeeper:2181'
#       CONTROL_CENTER_CONNECT_CLUSTER: 'connect:8083'
#       CONTROL_CENTER_REPLICATION_FACTOR: 1
#       CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
#       CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
#       CONFLUENT_METRICS_TOPIC_REPLICATION: 1
#       PORT: 9021
